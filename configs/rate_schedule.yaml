model_dir: ../tmp/rate_schedule

entry.class: AdvancedTrainer
entry.params:
  train_steps: 1000000
  save_checkpoint_steps: 1000
  summary_steps: 200
  output_dir: output
  csv_output_dir: output/training_data.csv
  model_output_dir: output/CTNMT_model
  criterion.class: LabelSmoothedCrossEntropy
  optimizer.class: adam
  optimizer_controller: RateScheduledOptimizer
  optimizer_controller_args:
    warm_steps: 10000
    freeze_steps: 20000
    controlled_varname_pattern: bert
  optimizer.params:
    epsilon: 1.e-9
    beta_1: 0.9
    beta_2: 0.98
  lr_schedule.class: noam
  lr_schedule.params:
    initial_factor: 1.0
    dmodel: 768
    warmup_steps: 4000
  freeze_variables: bert
  pretrain_model: ../tmp/BERT_BASE

dataset.class: ParallelTextDataset
dataset.params:
  src_file: tmp/data/train.en2am.in
  trg_file: tmp/data/train.en2am.out
  data_is_processed: false

task.class: Translation
task.params:
  src_data_pipeline.class: BertDataPipeline
  src_data_pipeline.params:
    name: bert-base-uncased
    vocab_path: tmp/vocab/vocab.txt
  trg_data_pipeline.class: TextDataPipeline
  trg_data_pipeline.params:
    subtokenizer: spm
    subtokenizer_codes: tmp/vocab/spm.model
    vocab_path: tmp/vocab/spm.vocab
  batch_size_per_gpu: 8000
  batch_by_tokens: true
  max_src_len: 100
  max_trg_len: 100

hparams_set: ctnmt_base
model.class: CtnmtTransformer
custom.model.class: CtnmtModel
# use BERT as encoder if you use rate scheduler only
model.params:
  bert_mode: bert_as_encoder

validator.class: SeqGenerationValidator
validator.params:
  eval_dataset: parallel_text
  eval_dataset.params:
    src_file: tmp/data/test.en2am.in
    trg_file: tmp/data/test.en2am.out
  eval_batch_size: 32
  eval_start_at: 1000
  eval_steps: 1000
  eval_criterion.class: label_smoothed_cross_entropy
  eval_search_method: beam_search
  eval_search_method.params:
    beam_size: 8
    length_penalty: 0.6
    extra_decode_length: 50
    maximum_decode_length: 200
    eval_metric: CompoundSplitBleu
  eval_top_checkpoints_to_keep: 10
  eval_auto_average_checkpoints: true
  eval_estop_patience: 30
